{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 2/8/2022 Text Vectorization\n",
    "#book: Applied Text Analysis with Python\n",
    "#authors: Benjamin Bengfort, Rebecca Bilbro, Tony Ojeda\n",
    "\n",
    "#infra: run on-prem \n",
    "#compute: my trainbox\n",
    "# created env anya_nlp_experiments: Python 3.7.6, networkx 2.4 already available in default base env, added gensim\n",
    "#      conda install -c conda-forge gensim\n",
    "#      gensim 3.8.3\n",
    "\n",
    "#2/8/2022 Text Vectorization\n",
    "#      Tokenization\n",
    "#      Frequency vectors with NLTK, Scikit-learn, Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://www.nltk.org/data.html\n",
    "\n",
    "NLTK comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: https://www.nltk.org/nltk_data/\n",
    "\n",
    "To install the data, first install NLTK (see https://www.nltk.org/install.html), then use NLTKâ€™s data downloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prerequisite\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### book `Applied Text Analysis with Python`\n",
    "Enabling language-aware data products with ML.  \n",
    "repo https://github.com/anyaconda/atap\n",
    "\n",
    "# Ch.4 Text Vectorization and Transformation Pipelines \n",
    "\n",
    "\n",
    "\n",
    "## Words in Space\n",
    "Setup: Create a list of docs and tokenize them for vectorization examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corpus object\n",
    "doc = \"The elephant sneezed at the sight of potatoes\"\n",
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization technique -> choice of implementation:  \n",
    "- NLTK\n",
    "- Scikit-Learn  \n",
    "- Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function - using NLTK\n",
    "#desc: perform lightweight normalization, strip punctuation, set to lowercase\n",
    "#return: tokens of type string\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "eleph\n",
      "sneez\n",
      "at\n",
      "the\n",
      "sight\n",
      "of\n",
      "potato\n"
     ]
    }
   ],
   "source": [
    "#my parts - Tokenization function\n",
    "for token in tokenize(doc):\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Vectors\n",
    "The simplest vector encoding model is to simply fill in the vector with the frequency of each work as it appears in a doc.  The representation can be either a straight count or a normalized encoding (each word is weighted by the total # of words in the doc. \n",
    "\n",
    "Formal definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_frequency_vectorize(corpus):\n",
    "\n",
    "    # The NLTK frequency vectorize method\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def vectorize(doc):\n",
    "        features = defaultdict(int)\n",
    "\n",
    "        for token in tokenize(doc):\n",
    "            features[token] += 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    return map(vectorize, corpus)\n",
    "\n",
    "\n",
    "def sklearn_frequency_vectorize(corpus):\n",
    "    # The Scikit-Learn frequency vectorize method\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "def gensim_frequency_vectorize(corpus):\n",
    "    # The Gensim frequency vectorize method\n",
    "    import gensim\n",
    "    \n",
    "    tokenized_corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    id2word = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    return [id2word.doc2bow(doc) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My parts\n",
    "\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NLTK frequency vectorize method\n",
    "from collections import defaultdict\n",
    "\n",
    "def vectorize(doc):\n",
    "    features = defaultdict(int)\n",
    "\n",
    "    for token in tokenize(doc):\n",
    "        features[token] += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'the': 2,\n",
       "             'eleph': 1,\n",
       "             'sneez': 1,\n",
       "             'at': 1,\n",
       "             'sight': 1,\n",
       "             'of': 1,\n",
       "             'potato': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my parts\n",
    "vectorize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<class 'int'>, {'the': 2, 'eleph': 1, 'sneez': 1, 'at': 1, 'sight': 1, 'of': 1, 'potato': 1}), defaultdict(<class 'int'>, {'bat': 2, 'can': 1, 'see': 2, 'via': 1, 'echoloc': 1, 'the': 1, 'sight': 1, 'sneez': 1}), defaultdict(<class 'int'>, {'wonder': 1, 'she': 1, 'open': 1, 'the': 2, 'door': 1, 'to': 1, 'studio': 1})]\n"
     ]
    }
   ],
   "source": [
    "vectors = map(vectorize, corpus) #class map\n",
    "print(list(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t2\n",
      "  (0, 6)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 10)\t2\n",
      "  (1, 18)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 13)\t1\n",
      "  (2, 16)\t2\n",
      "  (2, 19)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 15)\t1\n"
     ]
    }
   ],
   "source": [
    "#my parts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(corpus) #scipy.sparse.csr.csr_matrix\n",
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0]],\n",
       "       dtype=int64),\n",
       " array([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0]],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0].toarray(), dtm[1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'bat',\n",
       " 'bats',\n",
       " 'can',\n",
       " 'door',\n",
       " 'echolocation',\n",
       " 'elephant',\n",
       " 'of',\n",
       " 'opened',\n",
       " 'potatoes',\n",
       " 'see',\n",
       " 'she',\n",
       " 'sight',\n",
       " 'sneeze',\n",
       " 'sneezed',\n",
       " 'studio',\n",
       " 'the',\n",
       " 'to',\n",
       " 'via',\n",
       " 'wondering']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my parts\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "    \n",
    "tokenized_corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "id2word = gensim.corpora.Dictionary(tokenized_corpus) #class gensim.corpora.dictionary.Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato'],\n",
       " ['bat',\n",
       "  'can',\n",
       "  'see',\n",
       "  'via',\n",
       "  'echoloc',\n",
       "  'see',\n",
       "  'the',\n",
       "  'bat',\n",
       "  'sight',\n",
       "  'sneez'],\n",
       " ['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my parts\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)],\n",
       " [(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my parts\n",
    "[id2word.doc2bow(doc) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'at')\n",
      "(1, 'eleph')\n",
      "(2, 'of')\n",
      "(3, 'potato')\n",
      "(4, 'sight')\n",
      "(5, 'sneez')\n",
      "(6, 'the')\n",
      "(7, 'bat')\n",
      "(8, 'can')\n",
      "(9, 'echoloc')\n",
      "(10, 'see')\n",
      "(11, 'via')\n",
      "(12, 'door')\n",
      "(13, 'open')\n",
      "(14, 'she')\n",
      "(15, 'studio')\n",
      "(16, 'to')\n",
      "(17, 'wonder')\n"
     ]
    }
   ],
   "source": [
    "#my parts\n",
    "for item in id2word.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$xtra How does collections.defaultdict work?\n",
    "s = 'mississippi'\n",
    "d = defaultdict(int)\n",
    "for k in s:\n",
    "    d[k] += 1\n",
    "d.items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anya_nlp_experiments",
   "language": "python",
   "name": "anya_nlp_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
